{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "EsmtYkJqWcoK",
        "Jut0WuynY_nF",
        "uwNRcQE9deHK",
        "qDL9KHC9oNfn",
        "bwtihvA6s-6R",
        "i8JU8ADktZa0",
        "cqkDrCNH2pcW",
        "rfm__YjSuNk-",
        "4E1Anbt-yu_u",
        "C0gL1Mgpywir",
        "CCb2ICYKy6bD",
        "j6pReHK_4o0_",
        "ag5TKw-n5p74",
        "Th86CwmQ6LDB",
        "i1RgpFmG6fy_",
        "oU6LBlj7V_Hi",
        "wskT3pq5oV7G",
        "ye3f_bno7mFu",
        "2KtORXZn88_Q",
        "qtWa_wGAMy6N",
        "hBDCdSuEOltd",
        "USVP7qgsspFc"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Landslide susceptibility analysis of roads in Kyrgyzstan\n"
      ],
      "metadata": {
        "id": "lZ0pxzNLWY5V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "add text and short describtion"
      ],
      "metadata": {
        "id": "6np1joRk3dkQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparation, Data Download and Package Installing"
      ],
      "metadata": {
        "id": "EsmtYkJqWcoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2Pqwc5sWrNR",
        "outputId": "5451c709-fc2a-4ba8-a91d-f52105d70e25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install geopandas\n",
        "!pip install rasterio\n",
        "!pip install shapely\n",
        "!pip install gdal\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install rtree\n",
        "!pip install gdal\n",
        "!pip install scipy\n",
        "!pip install rioxarray\n",
        "!pip install netCDF4\n",
        "!pip install fiona\n",
        "!pip install dask rasterio dask[distributed]\n",
        "!pip install richdem\n",
        "!pip install xdem\n",
        "!pip install whitebox"
      ],
      "metadata": {
        "id": "0UcAIRI7cpSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "from shapely.geometry import Polygon, LineString, mapping, box\n",
        "import rasterio\n",
        "from rasterio.mask import mask\n",
        "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import requests\n",
        "import pandas as pd\n",
        "import base64\n",
        "import getopt\n",
        "import itertools\n",
        "import math\n",
        "import netrc\n",
        "import os\n",
        "import ssl\n",
        "import sys\n",
        "import time\n",
        "from getpass import getpass\n",
        "import xarray as xr\n",
        "import rioxarray\n",
        "import netCDF4\n",
        "import h5netcdf\n",
        "import fiona\n",
        "from pyproj import CRS, Transformer\n",
        "import zipfile\n",
        "from whitebox import WhiteboxTools\n",
        "import tempfile"
      ],
      "metadata": {
        "id": "ry2c7jy4crsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Downloads"
      ],
      "metadata": {
        "id": "Jut0WuynY_nF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "can be skipped, find the downloads in 00_input"
      ],
      "metadata": {
        "id": "V-BGiZqQWqFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# ----------------------------------------------------------------------------\n",
        "# NSIDC Data Download Script\n",
        "#\n",
        "# Copyright (c) 2024 Regents of the University of Colorado\n",
        "# Permission is hereby granted, free of charge, to any person obtaining\n",
        "# a copy of this software and associated documentation files (the \"Software\"),\n",
        "# to deal in the Software without restriction, including without limitation\n",
        "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
        "# and/or sell copies of the Software, and to permit persons to whom the\n",
        "# Software is furnished to do so, subject to the following conditions:\n",
        "# The above copyright notice and this permission notice shall be included\n",
        "# in all copies or substantial portions of the Software.\n",
        "#\n",
        "# Tested in Python 2.7 and Python 3.4, 3.6, 3.7, 3.8, 3.9\n",
        "#\n",
        "# To run the script at a Linux, macOS, or Cygwin command-line terminal:\n",
        "#   $ python nsidc-data-download.py\n",
        "#\n",
        "# On Windows, open Start menu -> Run and type cmd. Then type:\n",
        "#     python nsidc-data-download.py\n",
        "#\n",
        "# The script will first search Earthdata for all matching files.\n",
        "# You will then be prompted for your Earthdata username/password\n",
        "# and the script will download the matching files.\n",
        "#\n",
        "# If you wish, you may store your Earthdata username/password in a .netrc\n",
        "# file in your $HOME directory and the script will automatically attempt to\n",
        "# read this file. The .netrc file should have the following format:\n",
        "#    machine urs.earthdata.nasa.gov login MYUSERNAME password MYPASSWORD\n",
        "# where 'MYUSERNAME' and 'MYPASSWORD' are your Earthdata credentials.\n",
        "#\n",
        "# Instead of a username/password, you may use an Earthdata bearer token.\n",
        "# To construct a bearer token, log into Earthdata and choose \"Generate Token\".\n",
        "# To use the token, when the script prompts for your username,\n",
        "# just press Return (Enter). You will then be prompted for your token.\n",
        "# You can store your bearer token in the .netrc file in the following format:\n",
        "#    machine urs.earthdata.nasa.gov login token password MYBEARERTOKEN\n",
        "# where 'MYBEARERTOKEN' is your Earthdata bearer token.\n",
        "#\n",
        "from __future__ import print_function\n",
        "\n",
        "import base64\n",
        "import getopt\n",
        "import itertools\n",
        "import json\n",
        "import math\n",
        "import netrc\n",
        "import os.path\n",
        "import ssl\n",
        "import sys\n",
        "import time\n",
        "from getpass import getpass\n",
        "\n",
        "try:\n",
        "    from urllib.parse import urlparse\n",
        "    from urllib.request import urlopen, Request, build_opener, HTTPCookieProcessor\n",
        "    from urllib.error import HTTPError, URLError\n",
        "except ImportError:\n",
        "    from urlparse import urlparse\n",
        "    from urllib2 import urlopen, Request, HTTPError, URLError, build_opener, HTTPCookieProcessor\n",
        "\n",
        "short_name = 'HMA_DEM8m_MOS'\n",
        "version = '1'\n",
        "time_start = '2002-01-28T00:00:00Z'\n",
        "time_end = '2016-11-24T23:59:59Z'\n",
        "bounding_box = '68,39,81,44'\n",
        "polygon = ''\n",
        "filename_filter = ''\n",
        "url_list = []\n",
        "\n",
        "CMR_URL = 'https://cmr.earthdata.nasa.gov'\n",
        "URS_URL = 'https://urs.earthdata.nasa.gov'\n",
        "CMR_PAGE_SIZE = 2000\n",
        "CMR_FILE_URL = ('{0}/search/granules.json?provider=NSIDC_ECS'\n",
        "                '&sort_key[]=start_date&sort_key[]=producer_granule_id'\n",
        "                '&scroll=true&page_size={1}'.format(CMR_URL, CMR_PAGE_SIZE))\n",
        "\n",
        "\n",
        "def get_username():\n",
        "    username = ''\n",
        "\n",
        "    # For Python 2/3 compatibility:\n",
        "    try:\n",
        "        do_input = raw_input  # noqa\n",
        "    except NameError:\n",
        "        do_input = input\n",
        "\n",
        "    username = do_input('Earthdata username (or press Return to use a bearer token): ')\n",
        "    return username\n",
        "\n",
        "\n",
        "def get_password():\n",
        "    password = ''\n",
        "    while not password:\n",
        "        password = getpass('password: ')\n",
        "    return password\n",
        "\n",
        "\n",
        "def get_token():\n",
        "    token = ''\n",
        "    while not token:\n",
        "        token = getpass('bearer token: ')\n",
        "    return token\n",
        "\n",
        "\n",
        "def get_login_credentials():\n",
        "    \"\"\"Get user credentials from .netrc or prompt for input.\"\"\"\n",
        "    credentials = None\n",
        "    token = None\n",
        "\n",
        "    try:\n",
        "        info = netrc.netrc()\n",
        "        username, account, password = info.authenticators(urlparse(URS_URL).hostname)\n",
        "        if username == 'token':\n",
        "            token = password\n",
        "        else:\n",
        "            credentials = '{0}:{1}'.format(username, password)\n",
        "            credentials = base64.b64encode(credentials.encode('ascii')).decode('ascii')\n",
        "    except Exception:\n",
        "        username = None\n",
        "        password = None\n",
        "\n",
        "    if not username:\n",
        "        username = get_username()\n",
        "        if len(username):\n",
        "            password = get_password()\n",
        "            credentials = '{0}:{1}'.format(username, password)\n",
        "            credentials = base64.b64encode(credentials.encode('ascii')).decode('ascii')\n",
        "        else:\n",
        "            token = get_token()\n",
        "\n",
        "    return credentials, token\n",
        "\n",
        "\n",
        "def build_version_query_params(version):\n",
        "    desired_pad_length = 3\n",
        "    if len(version) > desired_pad_length:\n",
        "        print('Version string too long: \"{0}\"'.format(version))\n",
        "        quit()\n",
        "\n",
        "    version = str(int(version))  # Strip off any leading zeros\n",
        "    query_params = ''\n",
        "\n",
        "    while len(version) <= desired_pad_length:\n",
        "        padded_version = version.zfill(desired_pad_length)\n",
        "        query_params += '&version={0}'.format(padded_version)\n",
        "        desired_pad_length -= 1\n",
        "    return query_params\n",
        "\n",
        "\n",
        "def filter_add_wildcards(filter):\n",
        "    if not filter.startswith('*'):\n",
        "        filter = '*' + filter\n",
        "    if not filter.endswith('*'):\n",
        "        filter = filter + '*'\n",
        "    return filter\n",
        "\n",
        "\n",
        "def build_filename_filter(filename_filter):\n",
        "    filters = filename_filter.split(',')\n",
        "    result = '&options[producer_granule_id][pattern]=true'\n",
        "    for filter in filters:\n",
        "        result += '&producer_granule_id[]=' + filter_add_wildcards(filter)\n",
        "    return result\n",
        "\n",
        "\n",
        "def build_cmr_query_url(short_name, version, time_start, time_end,\n",
        "                        bounding_box=None, polygon=None,\n",
        "                        filename_filter=None):\n",
        "    params = '&short_name={0}'.format(short_name)\n",
        "    params += build_version_query_params(version)\n",
        "    params += '&temporal[]={0},{1}'.format(time_start, time_end)\n",
        "    if polygon:\n",
        "        params += '&polygon={0}'.format(polygon)\n",
        "    elif bounding_box:\n",
        "        params += '&bounding_box={0}'.format(bounding_box)\n",
        "    if filename_filter:\n",
        "        params += build_filename_filter(filename_filter)\n",
        "    return CMR_FILE_URL + params\n",
        "\n",
        "\n",
        "def get_speed(time_elapsed, chunk_size):\n",
        "    if time_elapsed <= 0:\n",
        "        return ''\n",
        "    speed = chunk_size / time_elapsed\n",
        "    if speed <= 0:\n",
        "        speed = 1\n",
        "    size_name = ('', 'k', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y')\n",
        "    i = int(math.floor(math.log(speed, 1000)))\n",
        "    p = math.pow(1000, i)\n",
        "    return '{0:.1f}{1}B/s'.format(speed / p, size_name[i])\n",
        "\n",
        "\n",
        "def output_progress(count, total, status='', bar_len=60):\n",
        "    if total <= 0:\n",
        "        return\n",
        "    fraction = min(max(count / float(total), 0), 1)\n",
        "    filled_len = int(round(bar_len * fraction))\n",
        "    percents = int(round(100.0 * fraction))\n",
        "    bar = '=' * filled_len + ' ' * (bar_len - filled_len)\n",
        "    fmt = '  [{0}] {1:3d}%  {2}   '.format(bar, percents, status)\n",
        "    print('\\b' * (len(fmt) + 4), end='')  # clears the line\n",
        "    sys.stdout.write(fmt)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "\n",
        "def cmr_read_in_chunks(file_object, chunk_size=1024 * 1024):\n",
        "    \"\"\"Read a file in chunks using a generator. Default chunk size: 1Mb.\"\"\"\n",
        "    while True:\n",
        "        data = file_object.read(chunk_size)\n",
        "        if not data:\n",
        "            break\n",
        "        yield data\n",
        "\n",
        "\n",
        "def get_login_response(url, credentials, token):\n",
        "    opener = build_opener(HTTPCookieProcessor())\n",
        "\n",
        "    req = Request(url)\n",
        "    if token:\n",
        "        req.add_header('Authorization', 'Bearer {0}'.format(token))\n",
        "    elif credentials:\n",
        "        try:\n",
        "            response = opener.open(req)\n",
        "            # We have a redirect URL - try again with authorization.\n",
        "            url = response.url\n",
        "        except HTTPError:\n",
        "            # No redirect - just try again with authorization.\n",
        "            pass\n",
        "        except Exception as e:\n",
        "            print('Error{0}: {1}'.format(type(e), str(e)))\n",
        "            sys.exit(1)\n",
        "\n",
        "        req = Request(url)\n",
        "        req.add_header('Authorization', 'Basic {0}'.format(credentials))\n",
        "\n",
        "    try:\n",
        "        response = opener.open(req)\n",
        "    except HTTPError as e:\n",
        "        err = 'HTTP error {0}, {1}'.format(e.code, e.reason)\n",
        "        if 'Unauthorized' in e.reason:\n",
        "            if token:\n",
        "                err += ': Check your bearer token'\n",
        "            else:\n",
        "                err += ': Check your username and password'\n",
        "        print(err)\n",
        "        sys.exit(1)\n",
        "    except Exception as e:\n",
        "        print('Error{0}: {1}'.format(type(e), str(e)))\n",
        "        sys.exit(1)\n",
        "\n",
        "    return response\n",
        "\n",
        "\n",
        "def cmr_download(urls, force=False, quiet=False):\n",
        "    \"\"\"Download files from list of urls.\"\"\"\n",
        "    if not urls:\n",
        "        return\n",
        "\n",
        "    url_count = len(urls)\n",
        "    if not quiet:\n",
        "        print('Downloading {0} files...'.format(url_count))\n",
        "    credentials = None\n",
        "    token = None\n",
        "    save_dir = '/content/drive/MyDrive/disaster_proj/tiff_files'\n",
        "    os.makedirs(save_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
        "\n",
        "    for index, url in enumerate(urls, start=1):\n",
        "        if not credentials and not token:\n",
        "            p = urlparse(url)\n",
        "            if p.scheme == 'https':\n",
        "                credentials, token = get_login_credentials()\n",
        "\n",
        "        filename = os.path.join(save_dir, url.split('/')[-1])\n",
        "        if not quiet:\n",
        "            print('{0}/{1}: {2}'.format(str(index).zfill(len(str(url_count))),\n",
        "                                        url_count, filename))\n",
        "\n",
        "        try:\n",
        "            response = get_login_response(url, credentials, token)\n",
        "            length = int(response.headers['content-length'])\n",
        "            try:\n",
        "                if not force and length == os.path.getsize(filename):\n",
        "                    if not quiet:\n",
        "                        print('  File exists, skipping')\n",
        "                    continue\n",
        "            except OSError:\n",
        "                pass\n",
        "            count = 0\n",
        "            chunk_size = min(max(length, 1), 1024 * 1024)\n",
        "            max_chunks = int(math.ceil(length / chunk_size))\n",
        "            time_initial = time.time()\n",
        "            with open(filename, 'wb') as out_file:\n",
        "                for data in cmr_read_in_chunks(response, chunk_size=chunk_size):\n",
        "                    out_file.write(data)\n",
        "                    if not quiet:\n",
        "                        count = count + 1\n",
        "                        time_elapsed = time.time() - time_initial\n",
        "                        download_speed = get_speed(time_elapsed, count * chunk_size)\n",
        "                        output_progress(count, max_chunks, status=download_speed)\n",
        "            if not quiet:\n",
        "                print()\n",
        "        except HTTPError as e:\n",
        "            print('HTTP error {0}, {1}'.format(e.code, e.reason))\n",
        "        except URLError as e:\n",
        "            print('URL error: {0}'.format(e.reason))\n",
        "        except IOError:\n",
        "            raise\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def cmr_filter_urls(search_results):\n",
        "    \"\"\"Select only the desired data files from CMR response.\"\"\"\n",
        "    if 'feed' not in search_results or 'entry' not in search_results['feed']:\n",
        "        return []\n",
        "\n",
        "    entries = [e['links']\n",
        "               for e in search_results['feed']['entry']\n",
        "               if 'links' in e]\n",
        "    # Flatten \"entries\" to a simple list of links\n",
        "    links = list(itertools.chain(*entries))\n",
        "\n",
        "    urls = []\n",
        "    unique_filenames = set()\n",
        "    for link in links:\n",
        "        if 'href' not in link:\n",
        "            # Exclude links with nothing to download\n",
        "            continue\n",
        "        if 'inherited' in link and link['inherited'] is True:\n",
        "            # Why are we excluding these links?\n",
        "            continue\n",
        "        if 'rel' in link and 'data#' not in link['rel']:\n",
        "            # Exclude links which are not classified by CMR as \"data\" or \"metadata\"\n",
        "            continue\n",
        "\n",
        "        if 'title' in link and 'opendap' in link['title'].lower():\n",
        "            # Exclude OPeNDAP links--they are responsible for many duplicates\n",
        "            # This is a hack; when the metadata is updated to properly identify\n",
        "            # non-datapool links, we should be able to do this in a non-hack way\n",
        "            continue\n",
        "\n",
        "        filename = link['href'].split('/')[-1]\n",
        "        if filename in unique_filenames:\n",
        "            # Exclude links with duplicate filenames (they would overwrite)\n",
        "            continue\n",
        "        unique_filenames.add(filename)\n",
        "\n",
        "        urls.append(link['href'])\n",
        "\n",
        "    return urls\n",
        "\n",
        "\n",
        "def cmr_search(short_name, version, time_start, time_end,\n",
        "               bounding_box='', polygon='', filename_filter='', quiet=False):\n",
        "    \"\"\"Perform a scrolling CMR query for files matching input criteria.\"\"\"\n",
        "    cmr_query_url = build_cmr_query_url(short_name=short_name, version=version,\n",
        "                                        time_start=time_start, time_end=time_end,\n",
        "                                        bounding_box=bounding_box,\n",
        "                                        polygon=polygon, filename_filter=filename_filter)\n",
        "    if not quiet:\n",
        "        print('Querying for data:\\n\\t{0}\\n'.format(cmr_query_url))\n",
        "\n",
        "    cmr_scroll_id = None\n",
        "    ctx = ssl.create_default_context()\n",
        "    ctx.check_hostname = False\n",
        "    ctx.verify_mode = ssl.CERT_NONE\n",
        "\n",
        "    urls = []\n",
        "    hits = 0\n",
        "    while True:\n",
        "        req = Request(cmr_query_url)\n",
        "        if cmr_scroll_id:\n",
        "            req.add_header('cmr-scroll-id', cmr_scroll_id)\n",
        "        try:\n",
        "            response = urlopen(req, context=ctx)\n",
        "        except Exception as e:\n",
        "            print('Error: ' + str(e))\n",
        "            sys.exit(1)\n",
        "        if not cmr_scroll_id:\n",
        "            # Python 2 and 3 have different case for the http headers\n",
        "            headers = {k.lower(): v for k, v in dict(response.info()).items()}\n",
        "            cmr_scroll_id = headers['cmr-scroll-id']\n",
        "            hits = int(headers['cmr-hits'])\n",
        "            if not quiet:\n",
        "                if hits > 0:\n",
        "                    print('Found {0} matches.'.format(hits))\n",
        "                else:\n",
        "                    print('Found no matches.')\n",
        "        search_page = response.read()\n",
        "        search_page = json.loads(search_page.decode('utf-8'))\n",
        "        url_scroll_results = cmr_filter_urls(search_page)\n",
        "        if not url_scroll_results:\n",
        "            break\n",
        "        if not quiet and hits > CMR_PAGE_SIZE:\n",
        "            print('.', end='')\n",
        "            sys.stdout.flush()\n",
        "        urls += url_scroll_results\n",
        "\n",
        "    if not quiet and hits > CMR_PAGE_SIZE:\n",
        "        print()\n",
        "    return urls\n",
        "\n",
        "\n",
        "def main(argv=None):\n",
        "    global short_name, version, time_start, time_end, bounding_box, \\\n",
        "        polygon, filename_filter, url_list\n",
        "\n",
        "    if argv is None:\n",
        "        argv = sys.argv[1:]\n",
        "\n",
        "    force = False\n",
        "    quiet = False\n",
        "    usage = 'usage: nsidc-download_***.py [--help, -h] [--force, -f] [--quiet, -q]'\n",
        "\n",
        "    try:\n",
        "        opts, args = getopt.getopt(argv, 'hfq', ['help', 'force', 'quiet'])\n",
        "        for opt, _arg in opts:\n",
        "            if opt in ('-f', '--force'):\n",
        "                force = True\n",
        "            elif opt in ('-q', '--quiet'):\n",
        "                quiet = True\n",
        "            elif opt in ('-h', '--help'):\n",
        "                print(usage)\n",
        "                sys.exit(0)\n",
        "    except getopt.GetoptError as e:\n",
        "        print(e.args[0])\n",
        "        print(usage)\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Supply some default search parameters, just for testing purposes.\n",
        "    # These are only used if the parameters aren't filled in up above.\n",
        "    if 'short_name' in short_name:\n",
        "        short_name = 'ATL06'\n",
        "        version = '003'\n",
        "        time_start = '2018-10-14T00:00:00Z'\n",
        "        time_end = '2021-01-08T21:48:13Z'\n",
        "        bounding_box = ''\n",
        "        polygon = ''\n",
        "        filename_filter = '*ATL06_2020111121*'\n",
        "        url_list = []\n",
        "\n",
        "    try:\n",
        "        if not url_list:\n",
        "            url_list = cmr_search(short_name, version, time_start, time_end,\n",
        "                                  bounding_box=bounding_box, polygon=polygon,\n",
        "                                  filename_filter=filename_filter, quiet=quiet)\n",
        "\n",
        "        cmr_download(url_list, force=force, quiet=quiet)\n",
        "    except KeyboardInterrupt:\n",
        "        quit()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "pr3lndf3dDOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7uA3_GFT3om"
      },
      "outputs": [],
      "source": [
        "output_file = \"/content/drive/MyDrive/disaster_proj/00_input/kg_admin/kyrgyzstan.geojson\"\n",
        "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "URL = \"https://api.ohsome.org/v1/elements/geometry\"\n",
        "filter_query = \"id:relation/178009\"\n",
        "timestamps = [\"2023-06-01\"]\n",
        "\n",
        "for timestamp in timestamps:\n",
        "    data = {\"bboxes\": None, \"time\": timestamp, \"filter\": filter_query}  # bboxes is not required for specific ID queries\n",
        "    response = requests.post(URL, data=data)\n",
        "\n",
        "    try:\n",
        "        response_json = response.json()\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error decoding JSON: {e}\")\n",
        "        continue\n",
        "\n",
        "    if 'features' in response_json:\n",
        "        features = response_json['features']\n",
        "        gdf = gpd.GeoDataFrame.from_features(features)\n",
        "\n",
        "        if gdf.empty:\n",
        "            print(\"GeoDataFrame is empty.\")\n",
        "            continue\n",
        "\n",
        "        gdf.set_crs(\"EPSG:4326\", inplace=True)\n",
        "        try:\n",
        "            gdf.to_file(output_file, driver=\"GeoJSON\")\n",
        "            print(f\"GeoDataFrame saved to output file {output_file}.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while saving the GeoDataFrame: {e}\")\n",
        "    else:\n",
        "        print(\"No features found in the response.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_file = \"/content/drive/MyDrive/disaster_proj/00_input/kg_admin/kg_roads.geojson\"\n",
        "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "\n",
        "URL = \"https://api.ohsome.org/v1/elements/geometry\"\n",
        "bbox = \"68.1557281, 39.0246074, 80.5633959, 43.5459749\"\n",
        "filters = [\"highway=* and type:way\"]\n",
        "timestamps = [\"2023-06-01\"]\n",
        "\n",
        "for filter in filters:\n",
        "    for timestamp in timestamps:\n",
        "        data = {\"bboxes\": bbox, \"time\": timestamp, \"filter\": filter}\n",
        "        response = requests.post(URL, data=data)\n",
        "\n",
        "        try:\n",
        "            response_json = response.json()\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Error decoding JSON: {e}\")\n",
        "            continue\n",
        "\n",
        "        if 'features' in response_json:\n",
        "            features = response_json['features']\n",
        "            gdf = gpd.GeoDataFrame.from_features(features)\n",
        "\n",
        "            if gdf.empty:\n",
        "                print(\"GeoDataFrame is empty.\")\n",
        "                continue\n",
        "\n",
        "            gdf.set_crs(\"EPSG:4326\", inplace=True)\n",
        "\n",
        "            gdf = gdf[gdf.geometry.type.isin([\"LineString\", \"MultiLineString\"])]\n",
        "            if gdf.empty:\n",
        "                print(\"No LineString or MultiLineString geometries found.\")\n",
        "                continue\n",
        "            try:\n",
        "                gdf.to_file(output_file, driver=\"GeoJSON\")\n",
        "                print(f\"GeoDataFrame saved to output file {output_file}.\")\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred while saving the GeoDataFrame: {e}\")\n",
        "        else:\n",
        "            print(\"No features found in the response.\")"
      ],
      "metadata": {
        "id": "bxT8jMbddMEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Data"
      ],
      "metadata": {
        "id": "9vwRvW6vdVA2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Reprojecting to 32642"
      ],
      "metadata": {
        "id": "uwNRcQE9deHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reproject_vector_to_epsg(input_file, output_file, epsg):\n",
        "  gdf = gpd.read_file(input_file)\n",
        "  gdf_reprojected = gdf.to_crs(epsg=epsg)\n",
        "  gdf_reprojected.to_file(output_file, driver='GeoJSON')\n",
        "  print(f'Reprojected vector file saved to {output_file}')"
      ],
      "metadata": {
        "id": "Xd-CDzU4dd6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reproject_raster(input_path, output_path, target_crs):\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "    if os.path.isfile(input_path):\n",
        "        try:\n",
        "            with rasterio.open(input_path) as src:\n",
        "                transform, width, height = calculate_default_transform(\n",
        "                    src.crs, target_crs, src.width, src.height, *src.bounds)\n",
        "                meta = src.meta.copy()\n",
        "                meta.update({\n",
        "                    'crs': target_crs,\n",
        "                    'transform': transform,\n",
        "                    'width': width,\n",
        "                    'height': height\n",
        "                })\n",
        "                name, ext = os.path.splitext(os.path.basename(input_path))\n",
        "                output_filename = os.path.join(output_path, f\"reprojected_{name}{ext}\")\n",
        "                with rasterio.open(output_filename, 'w', **meta) as dst:\n",
        "                    for i in range(1, src.count + 1):\n",
        "                        reproject(\n",
        "                            source=rasterio.band(src, i),\n",
        "                            destination=rasterio.band(dst, i),\n",
        "                            src_transform=src.transform,\n",
        "                            src_crs=src.crs,\n",
        "                            dst_transform=transform,\n",
        "                            dst_crs=target_crs,\n",
        "                            resampling=Resampling.nearest)\n",
        "        except Exception as e:\n",
        "            print(f\"Error reprojecting {input_path}: {e}\")\n",
        "    elif os.path.isdir(input_path):\n",
        "        search_pattern = os.path.join(input_path, '*.tif')\n",
        "        files = glob.glob(search_pattern)\n",
        "        for file in files:\n",
        "            try:\n",
        "                with rasterio.open(file) as src:\n",
        "                    transform, width, height = calculate_default_transform(\n",
        "                        src.crs, target_crs, src.width, src.height, *src.bounds)\n",
        "                    meta = src.meta.copy()\n",
        "                    meta.update({\n",
        "                        'crs': target_crs,\n",
        "                        'transform': transform,\n",
        "                        'width': width,\n",
        "                        'height': height\n",
        "                    })\n",
        "                    name, ext = os.path.splitext(os.path.basename(file))\n",
        "                    output_filename = os.path.join(output_path, f\"resampled_{name}{ext}\")\n",
        "                    with rasterio.open(output_filename, 'w', **meta) as dst:\n",
        "                        for i in range(1, src.count + 1):\n",
        "                            reproject(\n",
        "                                source=rasterio.band(src, i),\n",
        "                                destination=rasterio.band(dst, i),\n",
        "                                src_transform=src.transform,\n",
        "                                src_crs=src.crs,\n",
        "                                dst_transform=transform,\n",
        "                                dst_crs=target_crs,\n",
        "                                resampling=Resampling.nearest)\n",
        "            except Exception as e:\n",
        "                print(f\"Error reprojecting {file}: {e}\")\n",
        "    else:\n",
        "        print(\"Invalid input path.\")"
      ],
      "metadata": {
        "id": "Qlot39G-dXe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rain data needs special handling"
      ],
      "metadata": {
        "id": "b0Yvt1xcpIXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reproject_rain(input_path, output_path, target_crs):\n",
        "    nc_file = xr.open_dataset(input_path)\n",
        "    src_crs = '+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs +R=6371000'\n",
        "    nc_file.rio.write_crs(src_crs, inplace=True)\n",
        "    reprojected_data = nc_file['climatology-pr-annual-mean'].rio.reproject(target_crs, resampling=Resampling.bilinear)\n",
        "    reprojected_data.rio.to_raster(output_path)\n",
        "    print(\"Reprojection complete. Reprojected data saved to 'reprojected_rain.tif'\")"
      ],
      "metadata": {
        "id": "RF-TLH26nnGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clip to AOI\n",
        "later also used for clipping to road buffer\n"
      ],
      "metadata": {
        "id": "qDL9KHC9oNfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clip_geom_to_kg(input_geom, clip_to_geom, output_file):\n",
        "    input_geom = gpd.read_file(input_geom)\n",
        "    clip_area = gpd.read_file(clip_to_geom)\n",
        "    if input_geom.crs != clip_area.crs:\n",
        "        clip_area = clip_area.to_crs(input_geom.crs)\n",
        "    clipped_geom = gpd.clip(input_geom, clip_area)\n",
        "    output_dir = os.path.dirname(output_file)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    clipped_geom.to_file(output_file, driver='GeoJSON')\n",
        "    print(f'Clipped {input_geom} saved to {output_file}')"
      ],
      "metadata": {
        "id": "HFZbPY2AoNRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clip_raster_to_geom(input_path, boundary_path, output_path, value_range=(0, 10000)):\n",
        "    boundary = gpd.read_file(boundary_path)\n",
        "    shapes = [mapping(geometry) for geometry in boundary.geometry]\n",
        "    with rasterio.open(input_path) as src:\n",
        "        original_raster = src.read(1)\n",
        "        out_image, out_transform = mask(src, shapes, crop=True, all_touched=True)\n",
        "        out_meta = src.meta.copy()\n",
        "        nodata_value = src.nodata if src.nodata is not None else np.nan\n",
        "        out_image = np.where(out_image == nodata_value, np.nan, out_image)\n",
        "        min_val, max_val = value_range\n",
        "        out_image = np.clip(out_image, min_val, max_val)\n",
        "        if len(out_image.shape) > 2:\n",
        "            out_image = out_image[0]\n",
        "        out_meta.update({\n",
        "            \"driver\": \"GTiff\",\n",
        "            \"height\": out_image.shape[0],\n",
        "            \"width\": out_image.shape[1],\n",
        "            \"transform\": out_transform,\n",
        "            \"nodata\": nodata_value\n",
        "        })\n",
        "        with rasterio.Env(CHECK_DISK_FREE_SPACE=False): #big data\n",
        "          with rasterio.open(output_path, \"w\", **out_meta) as dest:\n",
        "              dest.write(out_image, 1)\n",
        "    print(f\"Clipped raster ({input_path}) saved to {output_path}\")"
      ],
      "metadata": {
        "id": "L8uumfR4oVHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clip_rasterfolder_to_geom(input_folder, boundary_path, output_folder, value_range=(0, 10000), default_nodata=-9999):\n",
        "    boundary = gpd.read_file(boundary_path)\n",
        "    shapes = [mapping(geometry) for geometry in boundary.geometry]\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    for input_path in glob.glob(os.path.join(input_folder, \"*.tif\")):\n",
        "        base_name = os.path.basename(input_path)\n",
        "        output_path = os.path.join(output_folder, f\"clipped_{base_name}\")\n",
        "\n",
        "        with rasterio.open(input_path) as src:\n",
        "            raster_bounds = box(*src.bounds)\n",
        "            raster_crs = src.crs\n",
        "\n",
        "            if boundary.crs != raster_crs:\n",
        "                boundary = boundary.to_crs(raster_crs)\n",
        "\n",
        "            overlap = boundary.geometry.intersects(raster_bounds)\n",
        "            if not any(overlap):\n",
        "                print(f\"No overlap between raster {base_name} and boundary. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            out_image, out_transform = mask(src, shapes, crop=True, all_touched=True)\n",
        "            out_meta = src.meta.copy()\n",
        "            nodata_value = src.nodata if src.nodata is not None else default_nodata\n",
        "\n",
        "            out_image = np.where(out_image == nodata_value, default_nodata, out_image)\n",
        "\n",
        "            min_val, max_val = value_range\n",
        "            out_image = np.clip(out_image, min_val, max_val)\n",
        "\n",
        "            if len(out_image.shape) > 2:\n",
        "                out_image = out_image[0]\n",
        "\n",
        "            out_meta.update({\n",
        "                \"driver\": \"GTiff\",\n",
        "                \"height\": out_image.shape[0],\n",
        "                \"width\": out_image.shape[1],\n",
        "                \"transform\": out_transform,\n",
        "                \"nodata\": default_nodata,\n",
        "                \"dtype\": \"float32\"\n",
        "            })\n",
        "\n",
        "            with rasterio.Env(CHECK_DISK_FREE_SPACE=False):  # For large data\n",
        "                with rasterio.open(output_path, \"w\", **out_meta) as dest:\n",
        "                    dest.write(out_image, 1)\n",
        "\n",
        "        print(f\"Clipped raster {input_path} saved to {output_path}\")\n",
        "\n",
        "    print(\"All rasters clipped and saved.\")"
      ],
      "metadata": {
        "id": "dZjzebHuLxaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Permaforst needs special handling and additional masking"
      ],
      "metadata": {
        "id": "tsOniUH-pDPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clip_pf_to_geom(input_path, boundary_path, output_path, value_range=(0, 10000)):\n",
        "    boundary = gpd.read_file(boundary_path)\n",
        "    shapes = [mapping(geometry) for geometry in boundary.geometry]\n",
        "    with rasterio.open(input_path) as src:\n",
        "        out_image, out_transform = mask(src, shapes, crop=True, all_touched=True)\n",
        "        out_meta = src.meta.copy()\n",
        "        nodata_value = src.nodata if src.nodata is not None else -9999\n",
        "        out_image = np.where(out_image == nodata_value, np.nan, out_image)\n",
        "        min_val, max_val = value_range\n",
        "        out_image = np.clip(out_image, min_val, max_val)\n",
        "        if out_image.ndim > 2:\n",
        "            out_image = out_image[0]\n",
        "        out_meta.update({\n",
        "            \"driver\": \"GTiff\",\n",
        "            \"height\": out_image.shape[0],\n",
        "            \"width\": out_image.shape[1],\n",
        "            \"transform\": out_transform,\n",
        "            \"nodata\": nodata_value\n",
        "        })\n",
        "        with rasterio.Env(CHECK_DISK_FREE_SPACE=False):\n",
        "            with rasterio.open(output_path, \"w\", **out_meta) as dest:\n",
        "                dest.write(out_image, 1)\n",
        "    print(f\"Clipped permafrost saved to {output_path}\")\n"
      ],
      "metadata": {
        "id": "_8E0_p-VoYCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_pf(input_path, output_path):\n",
        "    with rasterio.open(input_path) as src:\n",
        "        data = src.read(1)\n",
        "        meta = src.meta.copy()\n",
        "        data[data == 0] = -9999\n",
        "        meta.update({'nodata': -9999})\n",
        "        with rasterio.open(output_path, 'w', **meta) as dst:\n",
        "            dst.write(data, 1)\n",
        "    print(f\"Masked raster saved to {output_path}\")"
      ],
      "metadata": {
        "id": "ZjtLDhluoaOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Simplify and buffer roads"
      ],
      "metadata": {
        "id": "bwtihvA6s-6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import geopandas as gpd\n",
        "from shapely.ops import unary_union\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def buffer_roads(input_file, output_file):\n",
        "    os.environ['OGR_GEOJSON_MAX_OBJ_SIZE'] = '0'\n",
        "    roads = gpd.read_file(input_file)\n",
        "    print(\"File read successfully!\")\n",
        "    roads_simplified = roads.simplify(tolerance=500, preserve_topology=True)\n",
        "    roads_buffered = roads_simplified.buffer(3000)\n",
        "    merged_buffered_roads = unary_union(roads_buffered)\n",
        "    merged_gdf = gpd.GeoDataFrame(geometry=[merged_buffered_roads], crs=32642)\n",
        "    merged_gdf.to_file(output_file, driver='GeoJSON')\n",
        "    print(\"Merged buffered roads saved successfully!\")"
      ],
      "metadata": {
        "id": "U78NQxlctDns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##DEM derivates"
      ],
      "metadata": {
        "id": "xdIPBOSrtQiQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###calculate curvature from clipped DEM"
      ],
      "metadata": {
        "id": "i8JU8ADktZa0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_curvature(input_folder, output_folder):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    wbt = WhiteboxTools()\n",
        "    wbt.work_dir = input_folder\n",
        "\n",
        "    for dem_file in glob.glob(os.path.join(input_folder, \"*.tif\")):\n",
        "        output_file = os.path.join(output_folder, os.path.basename(dem_file).replace(\".tif\", \"_curvature.tif\"))\n",
        "\n",
        "        wbt.plan_curvature(dem_file, output_file, log=False, zfactor=None)\n",
        "        print(f\"Processed and saved: {dem_file} -> {output_file}\")\n",
        "\n",
        "    print(f\"All files processed and saved in folder: {output_folder}\")"
      ],
      "metadata": {
        "id": "DfmMJBl1SoRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###calculate slope from clipped DEM"
      ],
      "metadata": {
        "id": "ebgGhZAFtZ2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_slope(input_folder, output_folder):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    wbt = WhiteboxTools()\n",
        "    wbt.work_dir = input_folder\n",
        "\n",
        "    for dem_file in glob.glob(os.path.join(input_folder, \"*.tif\")):\n",
        "        output_file = os.path.join(output_folder, os.path.basename(dem_file).replace(\".tif\", \"_slope.tif\"))\n",
        "\n",
        "        wbt.slope(dem_file, output_file, units='DEGREES')  # You can change 'DEGREES' to 'PERCENT' if needed\n",
        "        print(f\"Processed and saved: {dem_file} -> {output_file}\")\n",
        "\n",
        "    print(f\"All files processed and saved in folder: {output_folder}\")"
      ],
      "metadata": {
        "id": "Liq41dCTTlUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merging"
      ],
      "metadata": {
        "id": "cqkDrCNH2pcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_rasters_gdal(input_dir, output_file):\n",
        "  os.makedirs(output_dir, exist_ok=True)\n",
        "    rasters = glob.glob(os.path.join(input_dir, \"*.tif\"))\n",
        "\n",
        "    if not rasters:\n",
        "        print(f\"No raster files found in {input_dir}. Skipping.\")\n",
        "        return\n",
        "\n",
        "    vrt_options = gdal.BuildVRTOptions(resampleAlg='bilinear', addAlpha=False)\n",
        "    vrt = gdal.BuildVRT('', rasters, options=vrt_options)\n",
        "\n",
        "    gdal.Translate(output_file, vrt, bandList=[1])\n",
        "\n",
        "    print(f\"Merged {len(rasters)} DEM rasters into {output_file}\")\n",
        "\n",
        "print(\"Merging completed!\")"
      ],
      "metadata": {
        "id": "NJsjEBBQTdSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_rasters_lulc(input_dir, category, output_path):\n",
        "    raster_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if category in f and f.endswith('.tif')]\n",
        "    if not raster_files:\n",
        "        print(f\"No files found for category: {category}\")\n",
        "        return\n",
        "\n",
        "    gdal_command = ['gdal_merge.py', '-o', output_path, '-n', '0', '-a_nodata', '0'] + raster_files\n",
        "    os.system(' '.join(gdal_command))\n",
        "    print(f\"Merged {category} files saved to {output_path}\")\n",
        "\n",
        "print(\"Merging completed!\")\n"
      ],
      "metadata": {
        "id": "kQ8iKT18vxRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Resample rasters"
      ],
      "metadata": {
        "id": "rfm__YjSuNk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resample_raster(input_path, output_path, target_resolution):\n",
        "  with rasterio.Env(CHECK_DISK_FREE_SPACE=False):\n",
        "    with rasterio.open(input_path) as src:\n",
        "        transform, width, height = calculate_default_transform(\n",
        "            src.crs, src.crs, src.width, src.height, *src.bounds, resolution=target_resolution\n",
        "        )\n",
        "        meta = src.meta.copy()\n",
        "        meta.update({\n",
        "            'crs': src.crs,\n",
        "            'transform': transform,\n",
        "            'width': width,\n",
        "            'height': height,\n",
        "            'nodata': -9999\n",
        "        })\n",
        "        with rasterio.open(output_path, 'w', **meta) as dst:\n",
        "            for band in range(1, src.count + 1):\n",
        "                reproject(\n",
        "                    source=rasterio.band(src, band),\n",
        "                    destination=rasterio.band(dst, band),\n",
        "                    src_transform=src.transform,\n",
        "                    src_crs=src.crs,\n",
        "                    dst_transform=transform,\n",
        "                    dst_crs=src.crs,\n",
        "                    resampling=Resampling.nearest\n",
        "                )"
      ],
      "metadata": {
        "id": "jTe35NOluPYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resample_raster_pf(input_path, output_path, target_resolution):\n",
        "    with rasterio.Env(CHECK_DISK_FREE_SPACE=False):\n",
        "        with rasterio.open(input_path) as src:\n",
        "            transform, width, height = calculate_default_transform(\n",
        "                src.crs, src.crs, src.width, src.height, *src.bounds, resolution=target_resolution)\n",
        "            meta = src.meta.copy()\n",
        "            meta.update({\n",
        "                'crs': src.crs,\n",
        "                'transform': transform,\n",
        "                'width': width,\n",
        "                'height': height,\n",
        "                'dtype': 'int16',\n",
        "                'nodata': -9999\n",
        "            })\n",
        "\n",
        "            with rasterio.open(output_path, 'w', **meta) as dst:\n",
        "                for band in range(1, src.count + 1):\n",
        "                    reproject(\n",
        "                        source=rasterio.band(src, band),\n",
        "                        destination=rasterio.band(dst, band),\n",
        "                        src_transform=src.transform,\n",
        "                        src_crs=src.crs,\n",
        "                        dst_transform=transform,\n",
        "                        dst_crs=src.crs,\n",
        "                        resampling=Resampling.nearest\n",
        "                    )\n",
        "\n",
        "    print(f\"Resampled raster saved to {output_path}\")"
      ],
      "metadata": {
        "id": "BXZubZtzv-lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resample_rasters_folder(input_folder, output_folder, target_resolution=30):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    for input_path in glob.glob(os.path.join(input_folder, \"*.tif\")):\n",
        "        base_name = os.path.basename(input_path)\n",
        "        output_path = os.path.join(output_folder, f\"resampled_{base_name}\")\n",
        "\n",
        "        with rasterio.Env(CHECK_DISK_FREE_SPACE=False):\n",
        "            with rasterio.open(input_path) as src:\n",
        "                transform, width, height = calculate_default_transform(\n",
        "                    src.crs, src.crs, src.width, src.height, *src.bounds, resolution=target_resolution\n",
        "                )\n",
        "                meta = src.meta.copy()\n",
        "                dtype = meta[\"dtype\"]\n",
        "                if dtype in [\"uint8\", \"uint16\"]:\n",
        "                    nodata_value = 0\n",
        "                elif dtype in [\"int16\", \"int32\"]:\n",
        "                    nodata_value = -9999\n",
        "                else:\n",
        "                    nodata_value = -9999.0\n",
        "\n",
        "                meta.update({\n",
        "                    'transform': transform,\n",
        "                    'width': width,\n",
        "                    'height': height,\n",
        "                    'nodata': nodata_value\n",
        "                })\n",
        "\n",
        "                with rasterio.open(output_path, 'w', **meta) as dst:\n",
        "                    for band in range(1, src.count + 1):\n",
        "                        reproject(\n",
        "                            source=rasterio.band(src, band),\n",
        "                            destination=rasterio.band(dst, band),\n",
        "                            src_transform=src.transform,\n",
        "                            src_crs=src.crs,\n",
        "                            dst_transform=transform,\n",
        "                            dst_crs=src.crs,\n",
        "                            resampling=Resampling.nearest\n",
        "                        )\n",
        "                print(f\"Resampled and saved: {output_path}\")\n",
        "    print(\"All files resampled and saved.\")"
      ],
      "metadata": {
        "id": "fm-HtNAiyRcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Classification"
      ],
      "metadata": {
        "id": "4E1Anbt-yu_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification is necessary, due to the common permafrost classification scheme. Continuous, discontinuous, sporadic and isolated permafrost."
      ],
      "metadata": {
        "id": "NAmqkBghNf6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reclassify_raster(input_file, output_file):\n",
        "    with rasterio.open(input_file) as src:\n",
        "        data = src.read(1)\n",
        "        no_data = src.nodata\n",
        "        if no_data is not None:\n",
        "            mask_nodata = data == no_data\n",
        "        else:\n",
        "            mask_nodata = np.zeros_like(data, dtype=bool)\n",
        "\n",
        "        reclassified_data = np.copy(data).astype(float)\n",
        "\n",
        "        reclassified_data[data <= 50] = 1.5  # isolated and sporadic\n",
        "        reclassified_data[(data > 50) & (data <= 90)] = 2.5  # discontinous\n",
        "        reclassified_data[data > 90] = 1.2  # continous\n",
        "\n",
        "        reclassified_data[mask_nodata] = no_data\n",
        "\n",
        "        profile = src.profile\n",
        "        profile.update(dtype=rasterio.float32)\n",
        "\n",
        "        with rasterio.Env(CHECK_DISK_FREE_SPACE=False):\n",
        "          with rasterio.open(output_file, 'w', **profile) as dst:\n",
        "              dst.write(reclassified_data.astype(rasterio.float32), 1)\n",
        "              print(f\"Reclassified raster saved to {output_file}\")"
      ],
      "metadata": {
        "id": "P4pPwR8ky1Cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Normalization and Interpolization"
      ],
      "metadata": {
        "id": "C0gL1Mgpywir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_lulc_file(input_file, output_folder):\n",
        "    raw_weights = {\n",
        "        'sparse_vegetation': -210.4,\n",
        "        'dense_forest': -284.1,\n",
        "        'snow': 6.53,\n",
        "        'water': -67.0,\n",
        "        'settlement': -20.5,\n",
        "        'barren_land': 185.1,\n",
        "        'fallow_land': 174.3,\n",
        "        'agriculture': 79.0\n",
        "    }\n",
        "\n",
        "    lulc_codes = {\n",
        "        1: 'water',              # Water\n",
        "        2: 'dense_forest',       # Dense forest\n",
        "        4: None,                 # Flooded vegetation (no class)\n",
        "        5: 'agriculture',        # Agriculture\n",
        "        7: 'settlement',         # Settlement\n",
        "        8: 'barren_land',        # Barren land\n",
        "        9: 'snow',               # Snow\n",
        "        10: None,                # Clouds (no data)\n",
        "        11: 'sparse_vegetation'  # Sparse vegetation\n",
        "    }\n",
        "\n",
        "    beta = 0.005\n",
        "\n",
        "    def normalize_weight(raw_weight, beta):\n",
        "        return math.exp(beta * raw_weight)\n",
        "\n",
        "    normalized_weights = {key: normalize_weight(value, beta) for key, value in raw_weights.items()}\n",
        "\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    output_path = os.path.join(output_folder, f'normalized_{os.path.basename(input_file)}')\n",
        "\n",
        "    with rasterio.open(input_file) as src:\n",
        "        lulc_data = src.read(1)\n",
        "        normalized_susceptibility = np.zeros(lulc_data.shape, dtype=np.float32)\n",
        "\n",
        "        for lulc_value, lulc_class in lulc_codes.items():\n",
        "            if lulc_class is not None:\n",
        "                normalized_susceptibility[lulc_data == lulc_value] = normalized_weights[lulc_class]\n",
        "\n",
        "        out_meta = src.meta.copy()\n",
        "        out_meta.update(dtype=rasterio.float32, count=1)\n",
        "\n",
        "    with rasterio.open(output_path, 'w', **out_meta) as dst:\n",
        "        dst.write(normalized_susceptibility, 1)\n",
        "\n",
        "    print(f\"Normalized LULC raster saved to {output_path}\")"
      ],
      "metadata": {
        "id": "iAL0dr1qy697"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_zip_if_needed(input_path, extraction_dir):\n",
        "    if input_path.endswith(\".zip\"):\n",
        "        with zipfile.ZipFile(input_path, \"r\") as zip_ref:\n",
        "            zip_ref.extractall(extraction_dir)\n",
        "            print(f\"Extracted {input_path} to {extraction_dir}\")\n",
        "        return extraction_dir\n",
        "    return input_path\n",
        "\n",
        "\n",
        "def interpolate_datapoints(data_type):\n",
        "    df = pd.read_csv('/content/drive/MyDrive/disaster_proj/00_input/data_points_for_landslide_density.csv')\n",
        "    x = df[df['type'] == data_type]['x'].values\n",
        "    y = df[df['type'] == data_type]['y'].values\n",
        "    pchip = PchipInterpolator(x, y)\n",
        "    return pchip\n",
        "\n",
        "\n",
        "def density_values(pchip, raster_input, output_file):\n",
        "    with rasterio.open(raster_input) as src:\n",
        "        raster_data = src.read(1)\n",
        "        raster_data = ma.masked_where(raster_data == src.nodata, raster_data)\n",
        "        landslide_density = pchip(raster_data)\n",
        "        landslide_density = ma.masked_where(raster_data.mask, landslide_density)\n",
        "\n",
        "        with rasterio.open(output_file, 'w', **src.meta) as dst:\n",
        "            dst.write(landslide_density, 1)\n",
        "            print(f\"Landslide density raster saved to {output_file}\")\n",
        "\n",
        "    return landslide_density\n",
        "\n",
        "\n",
        "def interpolate(input_paths, output_folder, data_types):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    for idx, input_path in enumerate(input_paths):\n",
        "        print(f\"Processing: {input_path}\")\n",
        "\n",
        "        temp_folder = f\"{output_folder}/temp_extracted_{idx}\"\n",
        "        processed_folder = extract_zip_if_needed(input_path, temp_folder)\n",
        "\n",
        "        try:\n",
        "            if input_path.endswith('clipped_to_road_buffer_rain.tif'):\n",
        "                output_file = os.path.join(output_folder, f\"precipitation_density.tif\")\n",
        "                pchip = interpolate_datapoints('precipitation')\n",
        "                density_values(pchip, processed_folder, output_file)\n",
        "                continue\n",
        "\n",
        "            raster_files = [\n",
        "                os.path.join(processed_folder, file)\n",
        "                for file in os.listdir(processed_folder) if file.endswith('.tif')\n",
        "            ] if os.path.isdir(processed_folder) else [processed_folder]\n",
        "\n",
        "            if not raster_files:\n",
        "                print(f\"No raster files found in {processed_folder}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            data_type = data_types[idx]\n",
        "\n",
        "            pchip = interpolate_datapoints(data_type)\n",
        "\n",
        "            for raster_file in raster_files:\n",
        "                base_name = os.path.basename(raster_file)\n",
        "                tile_number = base_name.split('_')[1].replace('tile-', '')\n",
        "\n",
        "                output_file = os.path.join(output_folder, f\"{data_type}_{base_name}\")\n",
        "\n",
        "                density_values(pchip, raster_file, output_file)\n",
        "\n",
        "        finally:\n",
        "            if os.path.exists(temp_folder):\n",
        "                shutil.rmtree(temp_folder)\n",
        "                print(f\"Temporary folder {temp_folder} deleted.\")"
      ],
      "metadata": {
        "id": "vYHqyOOtFCVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Landslide susceptibility\n",
        "Weight factors and calculate landslide susceptibility."
      ],
      "metadata": {
        "id": "CCb2ICYKy6bD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "import rasterio\n",
        "import numpy as np\n",
        "import glob\n",
        "\n",
        "def weighting(lulc_file, permafrost_file, density_folder, output_zip_file):\n",
        "    weights = {\n",
        "        \"slope\": 0.3,\n",
        "        \"permafrost\": 0.1,\n",
        "        \"precipitation\": 0.1,\n",
        "        \"elevation\": 0.3,\n",
        "        \"curvature\": 0.1,\n",
        "        \"lulc\": 0.1\n",
        "    }\n",
        "\n",
        "    output_temp_folder = \"/content/drive/MyDrive/disaster_proj/temp_outputs\"\n",
        "    os.makedirs(output_temp_folder, exist_ok=True)\n",
        "\n",
        "    def process_raster_in_chunks(input_file, weight, output_file):\n",
        "      print(f\"Processing file: {input_file} with weight: {weight}\")\n",
        "      with rasterio.open(input_file) as src:\n",
        "          meta = src.meta\n",
        "          meta.update(dtype=rasterio.float32)\n",
        "\n",
        "          nodata_value = src.nodata\n",
        "          data = src.read(1).astype(np.float32)\n",
        "\n",
        "          custom_nodata_values = [-9999, -999, -9999.0]\n",
        "          for val in custom_nodata_values:\n",
        "              data[data == val] = np.nan\n",
        "\n",
        "          if nodata_value is not None:\n",
        "              data[data == nodata_value] = np.nan\n",
        "\n",
        "          weighted_data = data * weight\n",
        "\n",
        "          print(f\"Value range of {input_file} after applying weight: min={np.nanmin(weighted_data)}, max={np.nanmax(weighted_data)}\")\n",
        "\n",
        "          with rasterio.open(output_file, 'w', **meta) as dst:\n",
        "              for ji, window in src.block_windows(1):\n",
        "                  block_data = src.read(1, window=window).astype(np.float32)\n",
        "\n",
        "                  for val in custom_nodata_values:\n",
        "                      block_data[block_data == val] = np.nan\n",
        "                  if nodata_value is not None:\n",
        "                      block_data[block_data == nodata_value] = np.nan\n",
        "\n",
        "                  weighted_block_data = block_data * weight\n",
        "                  dst.write(weighted_block_data.astype(rasterio.float32), 1, window=window)\n",
        "\n",
        "      print(f\"Saved weighted raster to: {output_file}\")\n",
        "\n",
        "    temp_files = []\n",
        "\n",
        "    print(\"Processing LULC file...\")\n",
        "    output_file = os.path.join(output_temp_folder, f\"weighted_{os.path.basename(lulc_file)}\")\n",
        "    process_raster_in_chunks(lulc_file, weights[\"lulc\"], output_file)\n",
        "    temp_files.append(output_file)\n",
        "\n",
        "    print(\"Processing Permafrost file...\")\n",
        "    output_file = os.path.join(output_temp_folder, f\"weighted_{os.path.basename(permafrost_file)}\")\n",
        "    process_raster_in_chunks(permafrost_file, weights[\"permafrost\"], output_file)\n",
        "    temp_files.append(output_file)\n",
        "\n",
        "    print(\"Processing Density files...\")\n",
        "    for key in [\"slope\", \"curvature\", \"precipitation\", \"elevation\"]:\n",
        "        density_files = glob.glob(os.path.join(density_folder, f\"*{key}*.tif\"))\n",
        "        print(f\"Found {len(density_files)} files for {key}.\")\n",
        "        for density_file in density_files:\n",
        "            output_file = os.path.join(output_temp_folder, f\"weighted_{os.path.basename(density_file)}\")\n",
        "            process_raster_in_chunks(density_file, weights[key], output_file)\n",
        "            temp_files.append(output_file)\n",
        "\n",
        "    print(\"Zipping output files...\")\n",
        "    with zipfile.ZipFile(output_zip_file, 'w') as zf:\n",
        "        for temp_file in temp_files:\n",
        "            zf.write(temp_file, arcname=os.path.basename(temp_file))\n",
        "            os.remove(temp_file)\n",
        "            print(f\"Deleted temporary file: {temp_file}\")\n",
        "\n",
        "    print(f\"All files have been processed and saved in: {output_zip_file}\")"
      ],
      "metadata": {
        "id": "3iTDiveZy7m3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "def unzip_file(input_zip_file, extract_to_folder):\n",
        "    os.makedirs(extract_to_folder, exist_ok=True)\n",
        "\n",
        "    with zipfile.ZipFile(input_zip_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to_folder)\n",
        "\n",
        "    print(f\"Files extracted to: {extract_to_folder}\")"
      ],
      "metadata": {
        "id": "VH69XNrucWir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fill nodata of permafrost with zero"
      ],
      "metadata": {
        "id": "2UQm_AlHtgbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import rasterio\n",
        "import numpy as np\n",
        "\n",
        "def fill_permafrost_with_ref_values(permafrost_file, reference_file, output_file):\n",
        "    with rasterio.open(permafrost_file) as permafrost_src, rasterio.open(reference_file) as reference_src:\n",
        "        permafrost_data = permafrost_src.read(1)\n",
        "        reference_data = reference_src.read(1)\n",
        "\n",
        "        mask_nan_permafrost = np.isnan(permafrost_data)\n",
        "        mask_valid_reference = ~np.isnan(reference_data)\n",
        "\n",
        "        permafrost_data[mask_nan_permafrost & mask_valid_reference] = 0\n",
        "\n",
        "        meta = permafrost_src.meta.copy()\n",
        "        meta.update(dtype=rasterio.float32, nodata=-999.0)\n",
        "\n",
        "    with rasterio.open(output_file, 'w', **meta) as output_src:\n",
        "        output_src.write(permafrost_data, 1)\n",
        "\n",
        "    print(f\"Filled permafrost raster saved to: {output_file}\")"
      ],
      "metadata": {
        "id": "E-5-H_Fxs61-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import rasterio\n",
        "from rasterio.warp import reproject, Resampling\n",
        "import numpy as np\n",
        "\n",
        "def align_and_sum_rasters(raster_files, output_file):\n",
        "    with rasterio.open(raster_files[0]) as ref_src:\n",
        "        ref_meta = ref_src.meta.copy()\n",
        "        ref_meta.update(dtype='float32')\n",
        "        ref_transform = ref_src.transform\n",
        "        ref_width = ref_src.width\n",
        "        ref_height = ref_src.height\n",
        "        ref_crs = ref_src.crs\n",
        "\n",
        "    sum_array = np.zeros((ref_height, ref_width), dtype=np.float32)\n",
        "\n",
        "    for raster_path in raster_files:\n",
        "        print(f\"Processing raster: {raster_path}\")\n",
        "        with rasterio.open(raster_path) as src:\n",
        "            aligned_data = np.zeros((ref_height, ref_width), dtype=np.float32)\n",
        "\n",
        "            reproject(\n",
        "                source=rasterio.band(src, 1),\n",
        "                destination=aligned_data,\n",
        "                src_transform=src.transform,\n",
        "                src_crs=src.crs,\n",
        "                dst_transform=ref_transform,\n",
        "                dst_crs=ref_crs,\n",
        "                resampling=Resampling.nearest,\n",
        "            )\n",
        "\n",
        "            sum_array += aligned_data\n",
        "\n",
        "    with rasterio.open(output_file, 'w', **ref_meta) as dst:\n",
        "        dst.write(sum_array, 1)\n",
        "\n",
        "    print(f\"Output saved to {output_file}\")\n"
      ],
      "metadata": {
        "id": "r8Xfi2tEtLf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Call functions in correct order"
      ],
      "metadata": {
        "id": "xkkwC8iRzBk4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execute"
      ],
      "metadata": {
        "id": "kE0jj1UG4uoR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Reproject"
      ],
      "metadata": {
        "id": "j6pReHK_4o0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###reproject vector"
      ],
      "metadata": {
        "id": "u5d9mr7x4tla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# admin boundary\n",
        "reproject_vector_to_epsg('/content/drive/MyDrive/disaster_proj/00_input/kg_admin/kyrgyzstan.geojson', '/content/drive/MyDrive/disaster_proj/01_reprojected/reprojected_kyrgyzstan.geojson', 32642)\n",
        "# roads\n",
        "reproject_vector_to_epsg('/content/drive/MyDrive/disaster_proj/00_input/kg_admin/kg_roads.geojson', '/content/drive/MyDrive/disaster_proj/01_reprojected/reprojected_kg_roads.geojson', 32642)"
      ],
      "metadata": {
        "id": "aRjha-0y4wxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###reproject raster"
      ],
      "metadata": {
        "id": "1pstvJK441WC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# permafrost\n",
        "reproject_raster('/content/drive/MyDrive/disaster_proj/00_input/permafrost.tif', '/content/drive/MyDrive/disaster_proj/01_reprojected/reprojected_permafrost', 32642)\n",
        "# reproject rain\n",
        "reproject_rain('/content/drive/MyDrive/disaster_proj/00_input/climatology-pr-annual-mean_cmip6-x0.25_hadgem3-gc31-mm-r1i1p1f3-historical_climatology_mean_1995-2014.nc', '/content/drive/MyDrive/disaster_proj/01_reprojected/reprojected_rain.tif', 32642)"
      ],
      "metadata": {
        "id": "Gg1HLsnh43Yf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Clip to Kyrgyzstan (AOI)"
      ],
      "metadata": {
        "id": "ag5TKw-n5p74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# roads\n",
        "clip_geom_to_kg('/content/drive/MyDrive/disaster_proj/01_reprojected/reprojected_kg_roads.geojson', '/content/drive/MyDrive/disaster_proj/01_reprojected/reprojected_kyrgyzstan.geojson', '/content/drive/MyDrive/disaster_proj/02_clipped_to_KG/clipped_roads.geojson')\n",
        "# rain\n",
        "clip_raster_to_geom('/content/drive/MyDrive/disaster_proj/01_reprojected/reprojected_rain.tif', '/content/drive/MyDrive/disaster_proj/01_reprojected/reprojected_kyrgyzstan.geojson', '/content/drive/MyDrive/disaster_proj/02_clipped_to_KG/clipped_rain.tif')"
      ],
      "metadata": {
        "id": "9P1THOZY5slG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Simplify and buffer roads"
      ],
      "metadata": {
        "id": "Th86CwmQ6LDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# buffer roads\n",
        "buffer_roads('/content/drive/MyDrive/disaster_proj/02_clipped_to_KG/clipped_roads.geojson', '/content/drive/MyDrive/disaster_proj/04_buffered_roads/merged_buffered_roads_3000.geojson')"
      ],
      "metadata": {
        "id": "3yafVzXQ6P-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. DEM derrivates"
      ],
      "metadata": {
        "id": "i1RgpFmG6fy_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Slope"
      ],
      "metadata": {
        "id": "lZNUvFRz6huq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "calc_slope(input_folder=\"/content/drive/MyDrive/disaster_proj/00_input/dem_files\", output_folder=\"/content/drive/MyDrive/disaster_proj/05_DEM_derivates/slope\")"
      ],
      "metadata": {
        "id": "mLrodwbY6jgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Curvature"
      ],
      "metadata": {
        "id": "K1EBgjZ36jug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "calc_curvature(input_folder=\"/content/drive/MyDrive/disaster_proj/00_input/dem_files\", output_folder=\"/content/drive/MyDrive/disaster_proj/05_DEM_derivates/curvature\")"
      ],
      "metadata": {
        "id": "qVBwZrZa6lPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Merge Raster files"
      ],
      "metadata": {
        "id": "oU6LBlj7V_Hi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_dir = '/content/drive/MyDrive/disaster_proj/05_DEM_derivates/curvature'\n",
        "output_dir = '/content/drive/MyDrive/disaster_proj/merged_rasters'\n",
        "output = os.path.join(output_dir, \"curvature_merged.tif\")\n",
        "merge_rasters_gdal(input_dir, output)"
      ],
      "metadata": {
        "id": "OHjr9DzHbXII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dir = '/content/drive/MyDrive/disaster_proj/05_DEM_derivates/slope'\n",
        "output_dir = '/content/drive/MyDrive/disaster_proj/merged_rasters'\n",
        "output = os.path.join(output_dir, \"slope_merged.tif\")\n",
        "merge_rasters_gdal(input_dir, output)"
      ],
      "metadata": {
        "id": "aanv_FxTnpiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dir = '/content/drive/MyDrive/disaster_proj/00_input/dem_files'\n",
        "output_dir = '/content/drive/MyDrive/disaster_proj/merged_rasters'\n",
        "output = os.path.join(output_dir, \"elevation_merged.tif\")\n",
        "merge_rasters_gdal(input_dir, output)"
      ],
      "metadata": {
        "id": "IRj9GPBcoDMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "density_dir = '/content/drive/MyDrive/disaster_proj/08_reclass_norm/density'\n",
        "lulc_dir = '/content/drive/MyDrive/disaster_proj/00_input/LULC_rasters'\n",
        "output_dir = '/content/drive/MyDrive/disaster_proj/merged_rasters'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "lulc_output = os.path.join(output_dir, \"lulc_merged.tif\")\n",
        "merge_rasters_lulc(lulc_dir, \"\", lulc_output)"
      ],
      "metadata": {
        "id": "pb3OpxsLnzAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Reproject Raster files"
      ],
      "metadata": {
        "id": "wskT3pq5oV7G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Curvature"
      ],
      "metadata": {
        "id": "6oy344mEp84P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reproject_raster('/content/drive/MyDrive/disaster_proj/merged_rasters/curvature_merged.tif', '/content/drive/MyDrive/disaster_proj/01_reprojected/reprojected_curvature', 32642)"
      ],
      "metadata": {
        "id": "UCl8Ye4ooViF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Slope"
      ],
      "metadata": {
        "id": "lFzJe3YBp-As"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reproject_raster('/content/drive/MyDrive/disaster_proj/merged_rasters/slope_merged.tif', '/content/drive/MyDrive/disaster_proj/01_reprojected/reprojected_slope', 32642)"
      ],
      "metadata": {
        "id": "MKCsUollouu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Elevation"
      ],
      "metadata": {
        "id": "o9_8946_p-oE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reproject_raster('/content/drive/MyDrive/disaster_proj/merged_rasters/elevation_merged.tif', '/content/drive/MyDrive/disaster_proj/01_reprojected/reprojected_elevation', 32642)"
      ],
      "metadata": {
        "id": "TxL6k6-8o0DV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LULC"
      ],
      "metadata": {
        "id": "F657Trqfp_xl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reproject_raster('/content/drive/MyDrive/disaster_proj/merged_rasters/lulc_merged.tif', '/content/drive/MyDrive/disaster_proj/01_reprojected/reprojected_lulc', 32642)"
      ],
      "metadata": {
        "id": "hr_Qjmfqo30M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precipitation"
      ],
      "metadata": {
        "id": "rAVj9uUOqAeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reproject_raster('/content/drive/MyDrive/disaster_proj/02_clipped_to_KG/clipped_rain.tif', '/content/drive/MyDrive/disaster_proj/01_reprojected/reprojected_rain', 32642)"
      ],
      "metadata": {
        "id": "j3qyq8YppyW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7. Resampling"
      ],
      "metadata": {
        "id": "ye3f_bno7mFu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEM"
      ],
      "metadata": {
        "id": "fgWS_BJp7uZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resample_raster('/content/drive/MyDrive/disaster_proj/01_reprojected/reprojected_elevation', '/content/drive/MyDrive/disaster_proj/07_resampled/resampled_elevation.tif', 30)"
      ],
      "metadata": {
        "id": "aF9n5plf7qb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Slope"
      ],
      "metadata": {
        "id": "pYY28LEFpIiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resample_raster('/content/drive/MyDrive/disaster_proj/01_reprojected/reprojected_slope', '/content/drive/MyDrive/disaster_proj/07_resampled/resampled_slope.tif', 30)"
      ],
      "metadata": {
        "id": "0QcHUy04pIL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Curvature"
      ],
      "metadata": {
        "id": "80spWbXOpJWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resample_raster('/content/drive/MyDrive/disaster_proj/01_reprojected/reprojected_curvature', '/content/drive/MyDrive/disaster_proj/07_resampled/resampled_curvature.tif', 30)"
      ],
      "metadata": {
        "id": "LbVS6kPOpH_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LULC"
      ],
      "metadata": {
        "id": "72TDQTIF7wF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resample_raster('/content/drive/MyDrive/disaster_proj/01_reprojected/reprojected_lulc', '/content/drive/MyDrive/disaster_proj/07_resampled/resampled_lulc.tif', 30)"
      ],
      "metadata": {
        "id": "F_BFR4HH7xlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precipitation"
      ],
      "metadata": {
        "id": "jL_rl72h7xTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resample_raster('/content/drive/MyDrive/disaster_proj/01_reprojected/reprojected_rain', '/content/drive/MyDrive/disaster_proj/07_resampled/resampled_rain.tif', 30)"
      ],
      "metadata": {
        "id": "jcpNrID77yWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Permafrost"
      ],
      "metadata": {
        "id": "IYkGrzbI72D4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resample_raster_pf('/content/drive/MyDrive/disaster_proj/01_reprojected/reprojected_permafrost.tif', '/content/drive/MyDrive/disaster_proj/07_resampled/resampled_permafrost.tif', 30)"
      ],
      "metadata": {
        "id": "BswS6i9i71hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##8. Clip to road buffer"
      ],
      "metadata": {
        "id": "2KtORXZn88_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Slope and Curvature"
      ],
      "metadata": {
        "id": "eSfRqjCK9Akc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Slope"
      ],
      "metadata": {
        "id": "upfKYXPP74A4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extract_and_process_zip('/content/drive/MyDrive/disaster_proj/05_DEM_derivates/slope_outputs.zip', '/content/drive/MyDrive/disaster_proj/04_buffered_roads/merged_buffered_roads_3000.geojson', '/content/drive/MyDrive/disaster_proj/06_clipped_to_road_buffer/clipped_slope')"
      ],
      "metadata": {
        "id": "9oUeQ0uu74Tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clip_raster_to_geom('/content/drive/MyDrive/disaster_proj/07_resampled/resampled_rain.tif', '/content/drive/MyDrive/disaster_proj/04_buffered_roads/merged_buffered_roads_3000.geojson', '/content/drive/MyDrive/disaster_proj/06_clipped_to_road_buffer/clipped_to_road_buffer_rain.tif' )"
      ],
      "metadata": {
        "id": "baoODCmeqjYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Curvature"
      ],
      "metadata": {
        "id": "gM4f6gFz75Uk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extract_and_process_zip('/content/drive/MyDrive/disaster_proj/05_DEM_derivates/curvature_outputs.zip', '/content/drive/MyDrive/disaster_proj/04_buffered_roads/merged_buffered_roads_3000.geojson', '/content/drive/MyDrive/disaster_proj/06_clipped_to_road_buffer/clipped_curvature')"
      ],
      "metadata": {
        "id": "uxRMoqcm76bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Everything else is clipped to the road buffer"
      ],
      "metadata": {
        "id": "vU205YT79mgO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEM"
      ],
      "metadata": {
        "id": "aZYd1Lab91-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clip_raster_to_geom('/content/drive/MyDrive/disaster_proj/07_resampled/resampled_elevation.tif', '/content/drive/MyDrive/disaster_proj/04_buffered_roads/merged_buffered_roads_3000.geojson', '/content/drive/MyDrive/disaster_proj/06_clipped_to_road_buffer/clipped_to_road_buffer_elevation.tif')"
      ],
      "metadata": {
        "id": "0ibnVrhZ9qDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Slope"
      ],
      "metadata": {
        "id": "x_ybecRqq63e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clip_raster_to_geom('/content/drive/MyDrive/disaster_proj/07_resampled/resampled_slope.tif', '/content/drive/MyDrive/disaster_proj/04_buffered_roads/merged_buffered_roads_3000.geojson', '/content/drive/MyDrive/disaster_proj/06_clipped_to_road_buffer/clipped_to_road_buffer_slope.tif')"
      ],
      "metadata": {
        "id": "3_xl7nwQq6XJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Curvature"
      ],
      "metadata": {
        "id": "iIAbH1Xtq7Xx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clip_raster_to_geom('/content/drive/MyDrive/disaster_proj/07_resampled/resampled_curvature.tif', '/content/drive/MyDrive/disaster_proj/04_buffered_roads/merged_buffered_roads_3000.geojson', '/content/drive/MyDrive/disaster_proj/06_clipped_to_road_buffer/clipped_to_road_buffer_curvature.tif')"
      ],
      "metadata": {
        "id": "-36_w-Fkq6Pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LULC"
      ],
      "metadata": {
        "id": "1YRwsXyP922n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clip_raster_to_geom('/content/drive/MyDrive/disaster_proj/07_resampled/resampled_lulc.tif', '/content/drive/MyDrive/disaster_proj/04_buffered_roads/merged_buffered_roads_3000.geojson', '/content/drive/MyDrive/disaster_proj/06_clipped_to_road_buffer/clipped_to_road_buffer_lulc.tif' )"
      ],
      "metadata": {
        "id": "kL5hjnS494EP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Permafrost"
      ],
      "metadata": {
        "id": "Lt6PqArD94Rh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clip_pf_to_geom('/content/drive/MyDrive/disaster_proj/07_resampled/resampled_permafrost.tif', '/content/drive/MyDrive/disaster_proj/04_buffered_roads/merged_buffered_roads_3000.geojson', '/content/drive/MyDrive/disaster_proj/06_clipped_to_road_buffer/clipped_to_road_buffer_permafrost.tif')"
      ],
      "metadata": {
        "id": "mZIG30Zw95td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask_pf('/content/drive/MyDrive/disaster_proj/06_clipped_to_road_buffer/clipped_to_road_buffer_permafrost.tif', '/content/drive/MyDrive/disaster_proj/06_clipped_to_road_buffer/clipped_to_road_buffer_permafrost_masked.tif')"
      ],
      "metadata": {
        "id": "y0_p1Svr9_UF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precipitation"
      ],
      "metadata": {
        "id": "4XorlnID96aP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clip_raster_to_geom('/content/drive/MyDrive/disaster_proj/07_resampled/resampled_rain.tif', '/content/drive/MyDrive/disaster_proj/04_buffered_roads/merged_buffered_roads_3000.geojson', '/content/drive/MyDrive/disaster_proj/06_clipped_to_road_buffer/clipped_to_road_buffer_rain.tif' )"
      ],
      "metadata": {
        "id": "DBEuR1hz98P9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##9. Classification"
      ],
      "metadata": {
        "id": "qtWa_wGAMy6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reclassify_raster('/content/drive/MyDrive/disaster_proj/06_clipped_to_road_buffer/clipped_to_road_buffer_permafrost_masked.tif', '/content/drive/MyDrive/disaster_proj/08_reclass_norm/permafrost/permafrost_reclassified.tif')"
      ],
      "metadata": {
        "id": "HlCCCKtoM8PA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##10. Normalization"
      ],
      "metadata": {
        "id": "hBDCdSuEOltd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalize_lulc_file('/content/drive/MyDrive/disaster_proj/06_clipped_to_road_buffer/clipped_to_road_buffer_lulc.tif', '/content/drive/MyDrive/disaster_proj/08_reclass_norm/lulc')"
      ],
      "metadata": {
        "id": "1wuk6vo_Oovb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_paths = [\n",
        "    '/content/drive/MyDrive/disaster_proj/06_clipped_to_road_buffer/clipped_to_road_buffer_elevation.tif',\n",
        "    '/content/drive/MyDrive/disaster_proj/06_clipped_to_road_buffer/clipped_to_road_buffer_slope.tif',\n",
        "    '/content/drive/MyDrive/disaster_proj/06_clipped_to_road_buffer/clipped_to_road_buffer_curvature.tif',\n",
        "    '/content/drive/MyDrive/disaster_proj/06_clipped_to_road_buffer/clipped_to_road_buffer_rain.tif'\n",
        "]\n",
        "\n",
        "data_types = ['elevation', 'slope', 'curvature', 'precipitation']\n",
        "\n",
        "output_folder = '/content/drive/MyDrive/disaster_proj/08_reclass_norm/density'\n",
        "\n",
        "interpolate(input_paths, output_folder, data_types)"
      ],
      "metadata": {
        "id": "vn1oM4I0KBnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weighting(\n",
        "    lulc_file='/content/drive/MyDrive/disaster_proj/08_reclass_norm/lulc/normalized_clipped_to_road_buffer_lulc.tif',\n",
        "    permafrost_file='/content/drive/MyDrive/disaster_proj/08_reclass_norm/permafrost/permafrost_reclassified.tif',\n",
        "    density_folder='/content/drive/MyDrive/disaster_proj/08_reclass_norm/density',\n",
        "    output_zip_file='/content/drive/MyDrive/disaster_proj/weighted_rasters.zip'\n",
        ")"
      ],
      "metadata": {
        "id": "HyRXVfyPbbIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Landslide Suscebility"
      ],
      "metadata": {
        "id": "USVP7qgsspFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unzip_file(\n",
        "    input_zip_file='/content/drive/MyDrive/disaster_proj/weighted_rasters.zip',\n",
        "    extract_to_folder='/content/drive/MyDrive/disaster_proj/temp_extracted'\n",
        ")"
      ],
      "metadata": {
        "id": "5PUpd-y0s0MC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fill_permafrost_with_ref_values(\n",
        "    permafrost_file='/content/drive/MyDrive/disaster_proj/temp_extracted/weighted_permafrost_reclassified.tif',\n",
        "    reference_file='/content/drive/MyDrive/disaster_proj/temp_extracted/weighted_slope_density.tif',\n",
        "    output_file='/content/drive/MyDrive/disaster_proj/temp_extracted/filled_permafrost.tif'\n",
        ")"
      ],
      "metadata": {
        "id": "_P5Spn2Fs94N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raster_files = [\n",
        "    \"/content/drive/MyDrive/disaster_proj/temp_extracted/filled_permafrost.tif\",\n",
        "    \"/content/drive/MyDrive/disaster_proj/temp_extracted/weighted_curvature_density.tif\",\n",
        "    \"/content/drive/MyDrive/disaster_proj/temp_extracted/weighted_elevation_density.tif\",\n",
        "    \"/content/drive/MyDrive/disaster_proj/temp_extracted/weighted_normalized_clipped_to_road_buffer_lulc.tif\",\n",
        "    \"/content/drive/MyDrive/disaster_proj/temp_extracted/weighted_precipitation_density.tif\",\n",
        "    \"/content/drive/MyDrive/disaster_proj/temp_extracted/weighted_slope_density.tif\",\n",
        "]\n",
        "output_file = \"/content/drive/MyDrive/disaster_proj/09_susceptibility/output.tif\"\n",
        "\n",
        "align_and_sum_rasters(raster_files, output_file)"
      ],
      "metadata": {
        "id": "H2z2ng0Ito6T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}